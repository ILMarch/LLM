{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1 - Set up kernel, load required dependencies, dataset, LLM"
      ],
      "metadata": {
        "id": "1Zod8u9AjJEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1 - Set up kernel and dependencies"
      ],
      "metadata": {
        "id": "SUjEfckNjJJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet"
      ],
      "metadata": {
        "id": "lHZJMiR1mrZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0XI06C8gmsJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2 - Load dataset and LLM"
      ],
      "metadata": {
        "id": "IgxQyYhLjJNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the Dialogsum HuggingFace dataset"
      ],
      "metadata": {
        "id": "fX2OTLi_L9K4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "o_cyQFwBmy-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pretrained FLAN-T5 model (small version) and its tokenizer from HF"
      ],
      "metadata": {
        "id": "aFT2qPooMFlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "dflYg8j_m3A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it."
      ],
      "metadata": {
        "id": "uwMrzrgam6sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "  trainable_model_params = 0\n",
        "  all_model_params = 0\n",
        "  for _, param in model.named_parameters():\n",
        "    all_model_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_model_params += param.numel()\n",
        "  return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model params: {(trainable_model_params/all_model_params)*100}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ],
      "metadata": {
        "id": "8i_BjXeim7Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3 - Test the model with zero-shot inferencing"
      ],
      "metadata": {
        "id": "8JO5daYgjJQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "# Input constructed prompt instead of the dialogue.\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '_'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ],
      "metadata": {
        "id": "5RHTaI9TnBmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2 - Perform full fine-tuning"
      ],
      "metadata": {
        "id": "uilnmI6ajJTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1 - Preprocess the dialog-summary dataset"
      ],
      "metadata": {
        "id": "0FJBvCfKjJWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with \"Summarize the following conversation\" and to the start of the summary with \"Summary\" as follows:"
      ],
      "metadata": {
        "id": "gKfuOQOiM2cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training prompt (dialogue):\n",
        "\n",
        "\n",
        "  Summarize the following conversation.\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "    Chris: blablabla\n",
        "    Ant: blablabla\n",
        "  Summary:\n",
        "\n",
        "Training response (summary):\n",
        "\n",
        ">\n",
        "\n",
        "    Both Chris and Ant participated in the conversation"
      ],
      "metadata": {
        "id": "_U1ac2BBnJHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we'll preprocess the prompt-response dataset into tokens and pull out their input_ids (1 per token)"
      ],
      "metadata": {
        "id": "t8n4_I-pNtYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "  start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "  end_prompt = '\\n\\nSummary: '\n",
        "  prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "  example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "  example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "  return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary', ])"
      ],
      "metadata": {
        "id": "2bsZSJQEnKOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To save some time in lab, we will subsample the dataset:\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "DyiYnZOAsj2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes of all three parts of the dataset:\n",
        "\n",
        "print(f\"Shapes f the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "o5BeNvJ6suPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output dataset is ready for fine-tuning"
      ],
      "metadata": {
        "id": "OUugpnfEUAbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 - Fine-tune the model with the preprocessed dataset"
      ],
      "metadata": {
        "id": "TWMddu1xjJY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we utilize the built-in HF Trainer class. Other parameters are found experimentally"
      ],
      "metadata": {
        "id": "NU8qrSdJUTZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "8q9QFx5es0WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainig a fully fine-tuned version of the model"
      ],
      "metadata": {
        "id": "SG9c-mogVWAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "8E61Qcb1tFho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a checkpoint of the fully fine-tuned model to use in the rest of this notebbook. This model aill also be referred to as the instruct model in this lab"
      ],
      "metadata": {
        "id": "lTwOxgtmVrhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/"
      ],
      "metadata": {
        "id": "jM--A07-tHqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh ./flan-dialogue-summary-checkpoint/pytorch_model.bin"
      ],
      "metadata": {
        "id": "SJ4mpXuwtHtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the instance of the AutoModelForSeq2SeqLM class for the instruct model:"
      ],
      "metadata": {
        "id": "C_Wr15b6WGSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "4tNKyS3ftHw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3 - Evaluate the model qualitatively (human evaluation)"
      ],
      "metadata": {
        "id": "kkB4QqoZjJbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
      ],
      "metadata": {
        "id": "U069GSm9tPpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.4 - Evaluate the model quantitatively (with ROUGE metric)"
      ],
      "metadata": {
        "id": "p3cVrW_UjJeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')"
      ],
      "metadata": {
        "id": "ITZAg3DXeFxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results"
      ],
      "metadata": {
        "id": "vyYOhAk3eMsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "  prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "  input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "\n",
        "  original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "  original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "  original_model_summaries.append(original_model_text_output)\n",
        "\n",
        "  instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "  instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "  instruct_model_summaries.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
        "df"
      ],
      "metadata": {
        "id": "C2IcS597eawF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the models computing ROUGE metrics"
      ],
      "metadata": {
        "id": "1Fcs4NGigJ1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL: ')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL: ')\n",
        "print(instruct_model_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "-6Dva_njgXcO",
        "outputId": "6b761c0c-635c-4d41-ef65-4bfcf8e3ccc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe3437c5cf8d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m original_model_results = rouge.compute(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_model_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuman_baseline_summaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_model_summaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0muse_aggregator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muse_stemmer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rouge' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show substantial improvement in all ROUGE metrics:"
      ],
      "metadata": {
        "id": "fBYN353ch0h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE')\n",
        "\n",
        "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(instruct_model_results.keys(), improvement):\n",
        "  print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "bFbT_Dk5h7yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3 - Perform parameter efficient fine-tuning (PEFT)"
      ],
      "metadata": {
        "id": "EYm2T3-7jJhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform Parameter Efficient Fine-Tuning (PEFT) as opposed full fine-tuning.\n",
        "\n",
        "PEFT is a generic term that includes Low-Rank Adaptation (LORA) and prompt-tuning."
      ],
      "metadata": {
        "id": "C4YOMw1ZjBgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1 - Setup the PEFT/LORA model for fine-tuning"
      ],
      "metadata": {
        "id": "CHB3byVwldi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set up the PEFT/LORA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LORA, we are freezing the underlying LLM and only training the adapter. In LORA configuration below, the rank (r) hyperparameter defines the rank/dimension of the adapter to be trained"
      ],
      "metadata": {
        "id": "J8Uy9a9kjlxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['q', 'v'],\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ],
      "metadata": {
        "id": "ezaseVhVkJce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add LORA adapter layers/params to the original LLM to be trained"
      ],
      "metadata": {
        "id": "DF2qfvsw-iuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model, lora_config)\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "mxKvGcrG-srk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###3.2 - Train PEFT adapter"
      ],
      "metadata": {
        "id": "aCGqKf9lld8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define training arguments and create Trainer instance"
      ],
      "metadata": {
        "id": "0MfMn2n2--xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets['train']\n",
        ")"
      ],
      "metadata": {
        "id": "uLT2PuNg_FWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path='./peft-dialogue-summary-checkpoint-local'\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "wPWy2MUoAQXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare this model by adding an adapter to the original FLAN-T5 model. We are setting is_trainable=False because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set is_trainable=True"
      ],
      "metadata": {
        "id": "icBpXTpmBMW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
        "                                       './peft-dialogue-summary-checkpoint-from-s3/',\n",
        "                                       torch_dtype=torch.bfloat16,\n",
        "                                       is_trainable=False)"
      ],
      "metadata": {
        "id": "AteVDyYzBxCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "iqoqKhIJCebS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3 - Evaluate the model qualitatively (human evaluation)"
      ],
      "metadata": {
        "id": "7opGst8gleGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL:\\n{peft_model_text_output}')"
      ],
      "metadata": {
        "id": "lWF6YBdCIz1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4 - Evaluate the model quantitatively (with ROUGE metric)"
      ],
      "metadata": {
        "id": "nEiNEX5oleRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "  prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "  input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "\n",
        "  human_baseline_text_output = human_baseline_summaries[idx]\n",
        "\n",
        "  original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "  original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "  original_model_summaries.append(original_model_text_output)\n",
        "\n",
        "  instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "  instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "  instruct_model_summaries.append(instruct_model_text_output)\n",
        "\n",
        "  peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "  peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "  peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
        "df"
      ],
      "metadata": {
        "id": "91rRYT1WJMt_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}